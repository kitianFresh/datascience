{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归(Linear regression)\n",
    "机器学习三要素, 模型 + 策略 + 算法;\n",
    "## 模型\n",
    "$x$ 表示一个输入样本的特征向量, 假设共有 $n$ 个 特征, 机器学习里我们默认向量是列向量, 这里就是一个 $n$ 维列向量; $w_i$ 就表示每一个特征 $x_i$ 对应的权重,构成一个权重列向量$w$; $b$ 表示偏置量;\n",
    "\n",
    "标量公式:\n",
    "$$\\hat{y}_t = \\sum_i{w_i*x_i} + b \\qquad (1)$$\n",
    "\n",
    "向量公式:\n",
    "$$\\hat{y}_t = w^Tx + b \\qquad (2)$$\n",
    "\n",
    "\n",
    "## 策略(损失函数)\n",
    "$y^t$ 表示第 t 个样本标签值, $\\hat{y}^t$ 是模型输出的预测值. 假设共有 m 个样本.由于房价预测这种预测值是实数空间,属于是回归问题,因此这里的损失函数选择常用的均方值误差MSE(Mean Squared Error);\n",
    "\n",
    "标量公式:\n",
    "$$L = \\frac{1}{2m}\\sum_t{(\\hat{y}^t - y^t)^2} \\qquad (3)$$\n",
    "\n",
    "向量公式:\n",
    "$$L = \\frac{1}{2m}(\\hat{y}-y)^T(\\hat{y}-y) \\qquad (4)$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "   y^1 \\\\\n",
    "   y^2 \\\\\n",
    "   \\vdots \\\\\n",
    "   y^t \\\\\n",
    "   \\vdots \\\\\n",
    "   y^m\n",
    "\\end{bmatrix} \\quad\n",
    "\\hat{y}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "   \\hat{y}^1 \\\\\n",
    "   \\hat{y}^2 \\\\\n",
    "   \\vdots \\\\\n",
    "   \\hat{y}^t \\\\\n",
    "   \\vdots \\\\\n",
    "   \\hat{y}^m\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "多样本堆叠公式:\n",
    "$$L = \\frac{1}{2m}(Xw+B-Y)^T(Xw+B-Y) ,Y=y\\qquad (5)$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Xw+B\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "   x^1_1 & x^1_2 & \\cdots & x^1_i & \\cdots & x^1_n \\\\\n",
    "   x^2_1 & x^2_2 & \\cdots & x^2_i & \\cdots & x^2_n \\\\\n",
    "   \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x^m_1 & x^m_2 & \\cdots & x^m_i & \\cdots & x^m_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   w_1 \\\\\n",
    "   w_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   w_i \\\\\n",
    "   \\vdots \\\\\n",
    "   w_n\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "   b \\\\\n",
    "   b \\\\\n",
    "   \\vdots \\\\\n",
    "   b \\\\\n",
    "   \\vdots \\\\\n",
    "   b\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## 算法(参数学习方法)\n",
    "我们的学习目标就是让损失函数最小, 而参数 w 和 b 是未知的, 就是求出使得 L 最小的 w 和 b. 我们知道在微积分中求解一个凸函数的最值一般就是在导数为 0 的地方. 而要找到导数为 0 的点, 通常我们数学上直接解方程式子就可以求出来了. 另一种方式就是梯度下降法, 这是遵循数学上沿着梯度方向下降最快的定理. 而梯度就是我们的导数了. 其实对于线性回归问题, 对于$X^TX$ 是可逆矩阵的时候,是可以直接性解出来的,不需要进行训练, 其中的 $X^T$ 表示所有样本列向量 $x^t$ 在水平方向堆叠起来的矩阵. 我们采用 BGD 进行训练. \n",
    "\n",
    "梯度:\n",
    "$$d_{w_i} = \\frac{1}{m}\\sum_t{(\\hat{y^t}-y^t)x_i} \\qquad (5)$$\n",
    "\n",
    "向量公式:\n",
    "$$d_{w} = $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, 0.0, 0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + math.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return (math.exp(z) - math.exp(-z)) / (math.exp(z) + math.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return max(0, z)\n",
    "\n",
    "def leaky_relu(rate, z):\n",
    "    return max(rate*z, z)\n",
    "print(sigmoid(0), tanh(0), relu(0), leaky_relu(0.01, 0))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def d_tanh(z):\n",
    "    return 1 - tanh(z)**2\n",
    "\n",
    "def d_relu(z):\n",
    "    return 0 if z <= 0 else 1\n",
    "    \n",
    "def d_leaky_relu(rate, z):\n",
    "    return rate if z <= 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑斯特回归(Logistic Regression)\n",
    "逻辑斯特回归其实是分类算法而不是回归算法, 他和 Linear Regression 的区别就是, 在做完计算之后, 再套一层激活函数,比如 sigmoid. \n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "![LR](./lr/lr-overview.png)\n",
    "\n",
    "## 模型\n",
    "$x$ 表示一个输入样本的特征向量, 假设共有 $n$ 个 特征, 机器学习里我们默认向量是列向量, 这里 $x$ 就是一个 $n$ 维列向量; $w_i$ 就表示每一个特征 $x_i$ 对应的权重,构成一个权重列向量$w$; $b$ 表示偏置量;$x_i^t$ 表示第 t 个样本的 i 分量;$a$ 表示输出值, 然后通过和阈值比较来判断属于哪一类, 一般是二分类.\n",
    "\n",
    "标量公式:\n",
    "$$z^t = \\sum_i{w_i*x_i^t} + b \\qquad (1)$$\n",
    "$$a^t = \\sigma(z^t)$$\n",
    "\n",
    "\n",
    "向量公式:\n",
    "$$z^t = w^Tx^t + b \\qquad (2)$$\n",
    "$$a^t = \\sigma(z^t)$$\n",
    "\n",
    "前向计算(forward compute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5252302562023774, 0.3339674909721524, 0.2687073716893833]\n",
      "[1, 1, 0, 0, 0]\n",
      "1.21776180878\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "m = 5\n",
    "n = 3\n",
    "b = 0.01\n",
    "L = .0\n",
    "z = [0]*m\n",
    "a = [0]*m\n",
    "w = [random.uniform(-1, 1) for _ in range(n)]\n",
    "print(w)\n",
    "x = [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9],\n",
    "     [10,11,12],\n",
    "     [13,14,15]]\n",
    "y = [random.randint(0, 1) for _ in range(m)]\n",
    "print(y)\n",
    "def dot(u, v):\n",
    "    n_u = len(u)\n",
    "    n_v = len(v)\n",
    "    assert n_u == n_v\n",
    "    sum = 0\n",
    "    for i in range(n_u):\n",
    "        sum += u[i] * v[i]\n",
    "    return sum\n",
    "# 使用了双重的 python 循环\n",
    "for t in range(m):\n",
    "    z[t] = dot(w, x[t]) + b\n",
    "    a[t] = sigmoid(z[t])\n",
    "    L += -(y[t]*math.log(a[t]) + (1-y[t])*math.log(1-a[t]))\n",
    "L/=m\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2177618087849436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m = 5\n",
    "n = 3\n",
    "b = 0.01\n",
    "L = .0\n",
    "z = [0]*m\n",
    "a = [0]*m\n",
    "w = np.asarray(w).reshape((n,1))\n",
    "assert w.shape == (n, 1)\n",
    "x = np.asmatrix(x).T\n",
    "assert x.shape == (n, m)\n",
    "# 这种是 python 循环和 numpy 混用的垃圾代码\n",
    "for t in range(m):\n",
    "    z[t] = np.dot(w.T, x[:,t]) + b\n",
    "    a[t] = sigmoid(z[t])\n",
    "    L += -(y[t]*np.log(a[t]) + (1-y[t])*np.log(1-a[t]))\n",
    "L /= m\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.83914449 -0.79119105 -1.17432944 -0.6069215 ]\n",
      " [-1.02534441 -0.0844406   0.93462087  1.30501676]\n",
      " [-0.34931739  1.22889803  0.25084471 -0.02991602]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2., -2., -2., -2.],\n",
       "       [-2., -2.,  1.,  1.],\n",
       "       [-2.,  1.,  1., -2.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return max(0, z)\n",
    "\n",
    "def leaky_relu(rate, z):\n",
    "    return max(rate*z, z)\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def d_tanh(z):\n",
    "    return 1 - tanh(z)**2\n",
    "\n",
    "def d_relu(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z<=0] = 0\n",
    "    return dz\n",
    "    \n",
    "def d_leaky_relu(rate, z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z<=0] = rate\n",
    "    return dz\n",
    "\n",
    "z = np.random.randn(3,4)\n",
    "print z\n",
    "d_relu(z)\n",
    "d_leaky_relu(-2, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4133842 ]\n",
      " [-0.51969963]\n",
      " [ 0.76912475]]\n",
      "[[ 1  4  7 10 13]\n",
      " [ 2  5  8 11 14]\n",
      " [ 3  6  9 12 15]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0.69368283]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m = 5\n",
    "n = 3\n",
    "B = 0.01\n",
    "L = .0\n",
    "x = [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9],\n",
    "     [10,11,12],\n",
    "     [13,14,15]]\n",
    "Z = np.zeros((1,m))\n",
    "A = np.zeros((1,m))\n",
    "# mu, sigma = 0, 0.1\n",
    "# w = np.random.normal(mu, sigma, n)\n",
    "W = np.asarray(w).reshape((n,1))\n",
    "assert W.shape == (n, 1)\n",
    "print(W)\n",
    "X = np.asmatrix(x).T\n",
    "assert X.shape == (n, m)\n",
    "print(X)\n",
    "Y = np.asarray(y).reshape((m, 1))\n",
    "assert Y.shape == (m, 1)\n",
    "print(Y)\n",
    "\n",
    "# 三行numpy 代码即可搞定前向传播 的numpy 优雅代码\n",
    "Z = np.dot(W.T, X) + B\n",
    "A = sigmoid(Z)\n",
    "func = sigmoid\n",
    "func.__name__\n",
    "L = -(np.dot(np.log(A), Y) + np.dot(np.log(1-A), 1-Y))\n",
    "L /= m\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 策略(损失函数)\n",
    "$y^t$ 表示第 t 个样本标签值, $a^t$ 是模型输出的预测值. 这里是分类问题,因此我们选择比较常用的交叉熵损失,也是极大似然估计损失; 注意,损失函数都是针对所有样本的评估;\n",
    "\n",
    "标量公式:\n",
    "$$L = -\\frac{1}{m}\\sum_t{y^t\\log{a^t} + (1-y^t)\\log(1-a^t)} \\qquad (3)$$\n",
    "\n",
    "\n",
    "## 算法(参数学习方法)\n",
    "我们的学习目标就是让损失函数最小, 而参数 w 和 b 是未知的, 就是求出使得 L 最小的 w 和 b. 我们知道在微积分中求解一个凸函数的最值一般就是在导数为 0 的地方. 而要找到导数为 0 的点, 通常我们数学上采用梯度下降法, 这是遵循数学上沿着梯度方向下降最快的定理. 而梯度就是我们的导数了.这里我们采用 BGD 进行训练. \n",
    "\n",
    "梯度:\n",
    "\n",
    "标量公式\n",
    "$$\n",
    "\\begin{align*}\n",
    "d_{w_i} &= \\frac{1}{m}\\sum_t{-(\\frac{\\partial L}{\\partial a^t}\\frac{\\partial a^t}{\\partial z^t}\\frac{\\partial z^t}{\\partial w_i})} & \\\\\n",
    "&= \\frac{1}{m}\\sum_t{-(\\frac{y^t}{a^t} - \\frac{1-y^t}{1-a^t})}a^t(1-a^t)x_i^t & \\\\\n",
    "&= \\frac{1}{m}\\sum_t(a^t-y^t)x_i^t \\qquad (5)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d_{b} &= \\frac{1}{m}\\sum_t{-(\\frac{\\partial L}{\\partial a^t}\\frac{\\partial a^t}{\\partial z^t}\\frac{\\partial z^t}{\\partial b})} & \\\\\n",
    "&= \\frac{1}{m}\\sum_t{-(\\frac{y^t}{a^t} - \\frac{1-y^t}{1-a^t})}a^t(1-a^t) & \\\\\n",
    "&= \\frac{1}{m}\\sum_t(a^t-y^t) \\qquad (6)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55695896]\n",
      " [-0.74966562]\n",
      " [ 0.31394363]]\n",
      "[1, 1, 0, 0, 0]\n",
      "1.08007605269212\n",
      "[array([ 4.13352667]), array([ 4.40021933]), array([ 4.666912])]\n",
      "[ 0.2884678]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "m = 5\n",
    "n = 3\n",
    "b = 0.01\n",
    "L = .0\n",
    "z = [0]*m\n",
    "a = [0]*m\n",
    "d_w = [0]*n\n",
    "d_b = random.uniform(0,1)\n",
    "print(w)\n",
    "x = [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9],\n",
    "     [10,11,12],\n",
    "     [13,14,15]]\n",
    "print(y)\n",
    "def dot(u, v):\n",
    "    n_u = len(u)\n",
    "    n_v = len(v)\n",
    "    assert n_u == n_v\n",
    "    sum = 0\n",
    "    for i in range(n_u):\n",
    "        sum += u[i] * v[i]\n",
    "    return sum\n",
    "# 使用了双重的 python 循环\n",
    "for t in range(m):\n",
    "    z[t] = dot(w, x[t]) + b\n",
    "    a[t] = sigmoid(z[t])\n",
    "    L += -(y[t]*math.log(a[t]) + (1-y[t])*math.log(1-a[t]))\n",
    "    for i in range(n):\n",
    "        d_w[i] += (a[t]-y[t])*x[t][i]\n",
    "    d_b += (a[t]-y[t])\n",
    "L /= m\n",
    "for i in range(n):\n",
    "    d_w[i] /= m\n",
    "d_b /= m\n",
    "print(L)\n",
    "print(d_w)\n",
    "print(d_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量公式:\n",
    "$$\n",
    "\\begin{align*}\n",
    "d_{w} &= \\frac{1}{m}\\sum_t{-(\\frac{\\partial L}{\\partial a^t}\\frac{\\partial a^t}{\\partial z^t}\\frac{\\partial z^t}{\\partial w})} & \\\\\n",
    "&= \\frac{1}{m}\\sum_t{-(\\frac{y^t}{a^t} - \\frac{1-y^t}{1-a^t})}a^t(1-a^t)x^t & \\\\\n",
    "&= \\frac{1}{m}\\sum_t(a^t-y^t)x^t \\qquad (7)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多样本的矩阵表示公式\n",
    "\n",
    "### 模型\n",
    "多样本堆叠公式:\n",
    "$$\n",
    "Z = W^TX + B \\qquad (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "W^TX+B\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "   w_1 & w_2 & \\cdots & w_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   x^1_1 & x^2_1 & \\cdots & x^i_1 & \\cdots & x^m_1 \\\\\n",
    "   x^1_2 & x^2_2 & \\cdots & x^i_2 & \\cdots & x^m_2 \\\\\n",
    "   \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x^1_n & x^2_n & \\cdots & x^i_n & \\cdots & x^m_n\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "   b & b & \\cdots & b\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A &=\n",
    "\\begin{bmatrix}\n",
    "  a^1 & a^2 & \\cdots & a^m  \n",
    "\\end{bmatrix} & \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\sigma(z^1) & \\sigma(z^2) & \\cdots & \\sigma(z^m) \n",
    "\\end{bmatrix} & \\\\\n",
    "&=\n",
    "\\sigma(Z)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$X$ 是由 m 个样本输入列向量 $x^i$ 水平堆叠得到的矩阵. 注意这里的 $Z$ $A$ $B$ 都是行向量的.\n",
    "\n",
    "使用向量堆叠, 现在的前向传播只需要一行代码,完全不需要显式的循环;\n",
    "\n",
    "### 策略\n",
    "多样本堆叠矩阵公式:\n",
    "$$\n",
    "L = -\\frac{1}{m}{\\log(A)*Y+ \\log(1-A)*(1-Y)} \\qquad (2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Y\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "   y^1 \\\\\n",
    "   y^2 \\\\\n",
    "   \\vdots \\\\\n",
    "   y^t \\\\\n",
    "   \\vdots \\\\\n",
    "   y^m\n",
    "\\end{bmatrix} \\quad\n",
    "A\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "   a^1 & a^2 & \\cdots & a^t & \\cdots a^m\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### 算法(梯度更新)\n",
    "多样本堆叠矩阵公式:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d_{a} &= \\frac{\\partial L}{\\partial A} & \\\\\n",
    "&= -\\frac{1}{m}(\\frac{Y^T}{A}-\\frac{1-Y^T}{1-A}) & \\\\\n",
    "&= \\frac{1}{m}{\\frac{A-Y^T}{A(1-A)}} \\qquad (3)\n",
    "\\end{align*}\n",
    "$$\n",
    "其中的 $A(1-A)$ 以及 $(A-Y^T)/A(1-A)$ 都是element-wise 的矩阵运算;\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d_{z} &= \\frac{\\partial L}{\\partial A}\\frac{\\partial A}{\\partial Z} & \\\\\n",
    "&= d_{a}A(1-A) & \\\\\n",
    "&= \\frac{1}{m}(A-Y^T) \\qquad (4)\n",
    "\\end{align*}\n",
    "$$\n",
    "消除后可以化简;\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d_{w} &= \\frac{\\partial L}{\\partial A}\\frac{\\partial A}{\\partial Z}\\frac{\\partial Z}{\\partial W} & \\\\\n",
    "&= Xd_z^T \\qquad (5)\n",
    "\\end{align*}\n",
    "$$\n",
    "最后,为了保持得到的倒导数矩阵的维度和$W$ 保持一致,我们必须对链式法则求导的结果运算进行转置和交换,来保证维度的一致性;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4133842 ]\n",
      " [-0.51969963]\n",
      " [ 0.76912475]]\n",
      "[[ 1  4  7 10 13]\n",
      " [ 2  5  8 11 14]\n",
      " [ 3  6  9 12 15]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "(1, 5)\n",
      "[[0.69368283]]\n",
      "[[2.62430403]\n",
      " [3.09770262]\n",
      " [3.57110121]]\n",
      "0.4733985918219136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = 5\n",
    "n = 3\n",
    "B = 0.01\n",
    "L = .0\n",
    "X = [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9],\n",
    "     [10,11,12],\n",
    "     [13,14,15]]\n",
    "# mu, sigma = 0, 0.1\n",
    "# w = np.random.normal(mu, sigma, n)\n",
    "W = np.asarray(w).reshape((n,1))\n",
    "assert W.shape == (n, 1)\n",
    "print(W)\n",
    "X = np.asmatrix(X).T\n",
    "assert X.shape == (n, m)\n",
    "print(X)\n",
    "Y = np.asarray(y).reshape((m, 1))\n",
    "assert Y.shape == (m, 1)\n",
    "print(Y)\n",
    "\n",
    "# 四行numpy 代码即可搞定前向传播 + 导数更新 的numpy 优雅代码\n",
    "Z = np.dot(W.T, X) + B\n",
    "A = sigmoid(Z)\n",
    "print(A.shape)\n",
    "L = -(np.dot(np.log(A), Y) + np.dot(np.log(1-A), 1-Y))\n",
    "d_z = A - Y.T\n",
    "d_w = np.dot(X, d_z.T)\n",
    "d_b = np.sum(d_z.T)\n",
    "L /= m\n",
    "d_w /= m\n",
    "d_b /= m\n",
    "print(L)\n",
    "print(d_w)\n",
    "print(d_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 浅层神经网络(shallow neural network)\n",
    "单隐藏层的神经网络可以看成是以逻辑斯特回归为基础神经节点, 再多加一层再次向前传播而已. 隐藏层的每一个单元就可以看成是一个逻辑斯特回归,\n",
    "![nn1](./nn/nn1.png)\n",
    "你可以把每个隐藏层节点都是一个逻辑斯特回归.\n",
    "![nn2](./nn/nn2.png)\n",
    "下面用公式来表示一个前向传播的过程:\n",
    "![nn3](./nn/nn3.png)\n",
    "![nn4](./nn/nn4.png)\n",
    "![nn-overview](./nn/nn-overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04150382  0.10106335  0.00486217]\n",
      " [ 0.09194814 -0.16178741  0.07048542]\n",
      " [-0.0677176   0.17172843 -0.08451493]\n",
      " [-0.12691324 -0.02429931  0.17965536]]\n",
      "[[-0.15748299 -0.24904317 -0.02985743 -0.06977265]]\n",
      "[[ 1  4  7 10 13]\n",
      " [ 2  5  8 11 14]\n",
      " [ 3  6  9 12 15]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "[[ 0.62201688]]\n",
      "[[ 0.24593593  0.24563482  0.24533314  0.2450359   0.24474764]]\n",
      "[[-0.29576876 -0.36447309 -0.15138725 -0.21224172]]\n",
      "[[-0.00682047  0.14438999  0.05683207]\n",
      " [ 0.15479425 -0.08451031  0.16219352]\n",
      " [-0.06026754  0.18090151 -0.07361884]\n",
      " [-0.1108408  -0.00440934  0.20336287]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sigmoid'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "m = 5\n",
    "n_0 = 3\n",
    "n_1 = 4\n",
    "n_2 = 1\n",
    "B1 = 0.01\n",
    "B2 = 0.01\n",
    "L = .0\n",
    "x = [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9],\n",
    "     [10,11,12],\n",
    "     [13,14,15]]\n",
    "mu, sigma = 0, 0.1\n",
    "W1 = np.random.normal(mu, sigma, (n_1, n_0))\n",
    "W2=  np.random.normal(mu, sigma, (n_2, n_1))\n",
    "assert W1.shape == (n_1, n_0)\n",
    "print(W1)\n",
    "assert W2.shape == (n_2, n_1)\n",
    "print(W2)\n",
    "X = np.asarray(x).T\n",
    "assert X.shape == (n_0, m)\n",
    "print(X)\n",
    "Y = np.random.randint(0,2, (m, 1))\n",
    "assert Y.shape == (m, 1)\n",
    "print(Y)\n",
    "\n",
    "# forword\n",
    "Z1 = np.dot(W1, X) + B1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.dot(W2, A1) + B2\n",
    "A2 = sigmoid(Z2)\n",
    "L = -(np.dot(np.log(A2), Y) + np.dot(np.log(1-A2), 1-Y))\n",
    "L/=m\n",
    "print(L)\n",
    "\n",
    "# backword\n",
    "\n",
    "print(A2 * (1 - A2))\n",
    "# d_A2 = 1/m *(A2-Y.T) / (A2*(1-A2)) 为了统一,这个 1/m 都放到最后除\n",
    "d_A2 = (A2-Y.T) / (A2*(1-A2))\n",
    "d_Z2 = d_A2 * d_sigmoid(Z2)\n",
    "d_W2 = np.dot(d_Z2, A1.T)\n",
    "d_B2 = d_Z2\n",
    "d_A1 = np.dot(W2.T, d_Z2)\n",
    "d_Z1 = d_A1 * d_sigmoid(Z1)\n",
    "d_W1 = np.dot(d_Z1, X.T)\n",
    "d_B1 = d_Z1\n",
    "\n",
    "W2 -= 1/m * d_W2\n",
    "B2 -= 1/m * d_B2\n",
    "W1 -= 1/m * d_W1\n",
    "B1 -= 1/m * d_B1\n",
    "\n",
    "print(W2)\n",
    "print(W1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4,5,6,7,8,9, 10, 11, 12])\n",
    "a = a.reshape((6,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 2],\n",
       "        [3, 4]]), array([[5, 6],\n",
       "        [7, 8]]), array([[ 9, 10],\n",
       "        [11, 12]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array_split(a, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = np.concatenate((*b[0:2], *b[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3, 4], \n",
    "                  [5, 6, 7, 8], \n",
    "                  [9, 10, 11, 12], \n",
    "                  [13, 14, 15, 16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  5, 10, 14])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0,0,1,1])\n",
    "index = [i for i in range(len(y))]\n",
    "s = a[index, y]\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4],\n",
       "       [ 6,  7,  8],\n",
       "       [10, 11, 12],\n",
       "       [14, 15, 16]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(a, [1,2,3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [1, 2, 3, 4],\n",
       "       [0, 1, 2, 3],\n",
       "       [0, 1, 2, 3]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.maximum(0, a - s.reshape((4,1)) + 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 3, 4],\n",
       "       [0, 2, 3, 4],\n",
       "       [0, 0, 2, 3],\n",
       "       [0, 0, 2, 3]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[index, y] = 0\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([6,7,8,9]).reshape((4,1))\n",
    "x + np.zeros((4,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 6, 6, 6],\n",
       "       [7, 7, 7, 7],\n",
       "       [8, 8, 8, 8],\n",
       "       [9, 9, 9, 9]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat = np.repeat(x, 4, axis=1)\n",
    "repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = t==0\n",
    "np.sum(w, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'LinearSVC': {u'C': 1.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 1.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 10.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 10.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 100.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 100.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 1.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 1.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 10.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 10.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 100.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 100.0,\n",
      "                 u'loss': u'hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 1.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 1.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 10.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 10.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 100.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 100.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l1'}},\n",
      " {u'LinearSVC': {u'C': 1.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 1.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 10.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 10.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 100.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'ovr',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LinearSVC': {u'C': 100.0,\n",
      "                 u'loss': u'squared_hinge',\n",
      "                 u'multi_class': u'crammer_singer',\n",
      "                 u'penalty': u'l2'}},\n",
      " {u'LogisticRegression': {u'C': 1.0,\n",
      "                          u'multi_class': u'ovr',\n",
      "                          u'penalty': u'l1'}},\n",
      " {u'LogisticRegression': {u'C': 1.0,\n",
      "                          u'multi_class': u'multinomial',\n",
      "                          u'penalty': u'l1'}},\n",
      " {u'LogisticRegression': {u'C': 10.0,\n",
      "                          u'multi_class': u'ovr',\n",
      "                          u'penalty': u'l1'}},\n",
      " {u'LogisticRegression': {u'C': 10.0,\n",
      "                          u'multi_class': u'multinomial',\n",
      "                          u'penalty': u'l1'}},\n",
      " {u'LogisticRegression': {u'C': 100.0,\n",
      "                          u'multi_class': u'ovr',\n",
      "                          u'penalty': u'l1'}},\n",
      " {u'LogisticRegression': {u'C': 100.0,\n",
      "                          u'multi_class': u'multinomial',\n",
      "                          u'penalty': u'l1'}},\n",
      " {u'LogisticRegression': {u'C': 1.0,\n",
      "                          u'multi_class': u'ovr',\n",
      "                          u'penalty': u'l2'}},\n",
      " {u'LogisticRegression': {u'C': 1.0,\n",
      "                          u'multi_class': u'multinomial',\n",
      "                          u'penalty': u'l2'}},\n",
      " {u'LogisticRegression': {u'C': 10.0,\n",
      "                          u'multi_class': u'ovr',\n",
      "                          u'penalty': u'l2'}},\n",
      " {u'LogisticRegression': {u'C': 10.0,\n",
      "                          u'multi_class': u'multinomial',\n",
      "                          u'penalty': u'l2'}},\n",
      " {u'LogisticRegression': {u'C': 100.0,\n",
      "                          u'multi_class': u'ovr',\n",
      "                          u'penalty': u'l2'}},\n",
      " {u'LogisticRegression': {u'C': 100.0,\n",
      "                          u'multi_class': u'multinomial',\n",
      "                          u'penalty': u'l2'}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 10,\n",
      "                              u'n_estimators': 10}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 10,\n",
      "                              u'n_estimators': 50}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 10,\n",
      "                              u'n_estimators': 100}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 50,\n",
      "                              u'n_estimators': 10}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 50,\n",
      "                              u'n_estimators': 50}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 50,\n",
      "                              u'n_estimators': 100}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 100,\n",
      "                              u'n_estimators': 10}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 100,\n",
      "                              u'n_estimators': 50}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 100,\n",
      "                              u'n_estimators': 100}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 10,\n",
      "                              u'n_estimators': 10}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 10,\n",
      "                              u'n_estimators': 50}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 10,\n",
      "                              u'n_estimators': 100}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 50,\n",
      "                              u'n_estimators': 10}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 50,\n",
      "                              u'n_estimators': 50}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 50,\n",
      "                              u'n_estimators': 100}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 100,\n",
      "                              u'n_estimators': 10}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 100,\n",
      "                              u'n_estimators': 50}},\n",
      " {u'RandomForestClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 100,\n",
      "                              u'n_estimators': 100}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'gini',\n",
      "                           u'max_depth': 10,\n",
      "                           u'splitter': u'best'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'gini',\n",
      "                           u'max_depth': 50,\n",
      "                           u'splitter': u'best'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'gini',\n",
      "                           u'max_depth': 100,\n",
      "                           u'splitter': u'best'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'gini',\n",
      "                           u'max_depth': 10,\n",
      "                           u'splitter': u'random'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'gini',\n",
      "                           u'max_depth': 50,\n",
      "                           u'splitter': u'random'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'gini',\n",
      "                           u'max_depth': 100,\n",
      "                           u'splitter': u'random'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'entropy',\n",
      "                           u'max_depth': 10,\n",
      "                           u'splitter': u'best'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'entropy',\n",
      "                           u'max_depth': 50,\n",
      "                           u'splitter': u'best'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'entropy',\n",
      "                           u'max_depth': 100,\n",
      "                           u'splitter': u'best'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'entropy',\n",
      "                           u'max_depth': 10,\n",
      "                           u'splitter': u'random'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'entropy',\n",
      "                           u'max_depth': 50,\n",
      "                           u'splitter': u'random'}},\n",
      " {u'ExtraTreeClassifier': {u'criterion': u'entropy',\n",
      "                           u'max_depth': 100,\n",
      "                           u'splitter': u'random'}},\n",
      " {u'PassiveAggressiveClassifier': {u'C': 1.0}},\n",
      " {u'PassiveAggressiveClassifier': {u'C': 10.0}},\n",
      " {u'PassiveAggressiveClassifier': {u'C': 100.0}},\n",
      " {u'SVC': {u'C': 1.0, u'kernel': u'linear'}},\n",
      " {u'SVC': {u'C': 1.0, u'kernel': u'rbf'}},\n",
      " {u'SVC': {u'C': 1.0, u'kernel': u'poly'}},\n",
      " {u'SVC': {u'C': 1.0, u'kernel': u'sigmoid'}},\n",
      " {u'SVC': {u'C': 10.0, u'kernel': u'linear'}},\n",
      " {u'SVC': {u'C': 10.0, u'kernel': u'rbf'}},\n",
      " {u'SVC': {u'C': 10.0, u'kernel': u'poly'}},\n",
      " {u'SVC': {u'C': 10.0, u'kernel': u'sigmoid'}},\n",
      " {u'SVC': {u'C': 100.0, u'kernel': u'linear'}},\n",
      " {u'SVC': {u'C': 100.0, u'kernel': u'rbf'}},\n",
      " {u'SVC': {u'C': 100.0, u'kernel': u'poly'}},\n",
      " {u'SVC': {u'C': 100.0, u'kernel': u'sigmoid'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 10,\n",
      "                              u'splitter': u'best'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 50,\n",
      "                              u'splitter': u'best'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 100,\n",
      "                              u'splitter': u'best'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 10,\n",
      "                              u'splitter': u'random'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 50,\n",
      "                              u'splitter': u'random'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'gini',\n",
      "                              u'max_depth': 100,\n",
      "                              u'splitter': u'random'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 10,\n",
      "                              u'splitter': u'best'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 50,\n",
      "                              u'splitter': u'best'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 100,\n",
      "                              u'splitter': u'best'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 10,\n",
      "                              u'splitter': u'random'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 50,\n",
      "                              u'splitter': u'random'}},\n",
      " {u'DecisionTreeClassifier': {u'criterion': u'entropy',\n",
      "                              u'max_depth': 100,\n",
      "                              u'splitter': u'random'}},\n",
      " {u'GaussianNB': {u'priors': [0.1,\n",
      "                              0.1,\n",
      "                              0.1,\n",
      "                              0.1,\n",
      "                              0.1,\n",
      "                              0.1,\n",
      "                              0.1,\n",
      "                              0.1,\n",
      "                              0.1,\n",
      "                              0.1]}},\n",
      " {u'SGDClassifier': {u'loss': u'hinge', u'penalty': u'l1'}},\n",
      " {u'SGDClassifier': {u'loss': u'hinge', u'penalty': u'l2'}},\n",
      " {u'SGDClassifier': {u'loss': u'hinge', u'penalty': u'elasticnet'}},\n",
      " {u'SGDClassifier': {u'loss': u'squared_hinge', u'penalty': u'l1'}},\n",
      " {u'SGDClassifier': {u'loss': u'squared_hinge', u'penalty': u'l2'}},\n",
      " {u'SGDClassifier': {u'loss': u'squared_hinge', u'penalty': u'elasticnet'}},\n",
      " {u'SGDClassifier': {u'loss': u'perceptron', u'penalty': u'l1'}},\n",
      " {u'SGDClassifier': {u'loss': u'perceptron', u'penalty': u'l2'}},\n",
      " {u'SGDClassifier': {u'loss': u'perceptron', u'penalty': u'elasticnet'}},\n",
      " {u'SGDClassifier': {u'loss': u'log', u'penalty': u'l1'}},\n",
      " {u'SGDClassifier': {u'loss': u'log', u'penalty': u'l2'}},\n",
      " {u'SGDClassifier': {u'loss': u'log', u'penalty': u'elasticnet'}},\n",
      " {u'SGDClassifier': {u'loss': u'modified_huber', u'penalty': u'l1'}},\n",
      " {u'SGDClassifier': {u'loss': u'modified_huber', u'penalty': u'l2'}},\n",
      " {u'SGDClassifier': {u'loss': u'modified_huber', u'penalty': u'elasticnet'}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 3,\n",
      "                                  u'n_estimators': 10}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 10,\n",
      "                                  u'n_estimators': 10}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 50,\n",
      "                                  u'n_estimators': 10}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 3,\n",
      "                                  u'n_estimators': 50}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 10,\n",
      "                                  u'n_estimators': 50}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 50,\n",
      "                                  u'n_estimators': 50}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 3,\n",
      "                                  u'n_estimators': 100}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 10,\n",
      "                                  u'n_estimators': 100}},\n",
      " {u'GradientBoostingClassifier': {u'loss': u'deviance',\n",
      "                                  u'max_depth': 50,\n",
      "                                  u'n_estimators': 100}},\n",
      " {u'Perceptron': {u'penalty': u'l1'}},\n",
      " {u'Perceptron': {u'penalty': u'l2'}},\n",
      " {u'Perceptron': {u'penalty': u'elasticnet'}},\n",
      " {u'GaussianProcessClassifier': {u'multi_class': u'one_vs_one'}},\n",
      " {u'GaussianProcessClassifier': {u'multi_class': u'one_vs_rest'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 1,\n",
      "                            u'p': 1,\n",
      "                            u'weights': u'uniform'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 1,\n",
      "                            u'p': 2,\n",
      "                            u'weights': u'uniform'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 1,\n",
      "                            u'p': 1,\n",
      "                            u'weights': u'distance'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 1,\n",
      "                            u'p': 2,\n",
      "                            u'weights': u'distance'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 5,\n",
      "                            u'p': 1,\n",
      "                            u'weights': u'uniform'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 5,\n",
      "                            u'p': 2,\n",
      "                            u'weights': u'uniform'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 5,\n",
      "                            u'p': 1,\n",
      "                            u'weights': u'distance'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 5,\n",
      "                            u'p': 2,\n",
      "                            u'weights': u'distance'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 9,\n",
      "                            u'p': 1,\n",
      "                            u'weights': u'uniform'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 9,\n",
      "                            u'p': 2,\n",
      "                            u'weights': u'uniform'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 9,\n",
      "                            u'p': 1,\n",
      "                            u'weights': u'distance'}},\n",
      " {u'KNeighborsClassifier': {u'n_neighbors': 9,\n",
      "                            u'p': 2,\n",
      "                            u'weights': u'distance'}},\n",
      " {u'MLPClassifier': {u'activation': u'tanh', u'hidden_layer_sizes': u'(10,)'}},\n",
      " {u'MLPClassifier': {u'activation': u'tanh',\n",
      "                     u'hidden_layer_sizes': u'(100,)'}},\n",
      " {u'MLPClassifier': {u'activation': u'tanh',\n",
      "                     u'hidden_layer_sizes': u'(10,10,)'}},\n",
      " {u'MLPClassifier': {u'activation': u'tanh',\n",
      "                     u'hidden_layer_sizes': u'(100,10,)'}},\n",
      " {u'MLPClassifier': {u'activation': u'relu', u'hidden_layer_sizes': u'(10,)'}},\n",
      " {u'MLPClassifier': {u'activation': u'relu',\n",
      "                     u'hidden_layer_sizes': u'(100,)'}},\n",
      " {u'MLPClassifier': {u'activation': u'relu',\n",
      "                     u'hidden_layer_sizes': u'(10,10,)'}},\n",
      " {u'MLPClassifier': {u'activation': u'relu',\n",
      "                     u'hidden_layer_sizes': u'(100,10,)'}}]\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "import random\n",
    "from pprint import pprint\n",
    "from ast import literal_eval as make_tuple\n",
    "class JM:\n",
    "    def __init__(self, do_shuffle=False):\n",
    "        self.do_shuffle = do_shuffle\n",
    "        \n",
    "    def _parse_list(self, v): # for mlp\n",
    "        \n",
    "        for idx, vv in enumerate(v):\n",
    "            if isinstance(vv, str) and vv.startswith('('):\n",
    "                v[idx] = make_tuple(vv)\n",
    "        return v\n",
    "\n",
    "    def _parse_tasks(self, fn):\n",
    "        with open(fn) as fp:\n",
    "            tmp = json.load(fp)\n",
    "\n",
    "        def get_par_comb(tmp, clf_name):\n",
    "            all_par_vals = list(itertools.product(*[self._parse_list(vv)\n",
    "                                                    for v in tmp['classifiers'][clf_name]\n",
    "                                                    for vv in v.values()]))\n",
    "            all_par_name = [vv for v in tmp['classifiers'][clf_name] for vv in v.keys()]\n",
    "            return [{all_par_name[idx]: vv for idx, vv in enumerate(v)} for v in all_par_vals]\n",
    "\n",
    "        result = [{v: vv} for v in tmp['classifiers'] for vv in get_par_comb(tmp, v)]\n",
    "        for v in result:\n",
    "            for vv in v.values():\n",
    "                vv.update(tmp['common'])\n",
    "        if self.do_shuffle:\n",
    "            random.shuffle(result)\n",
    "        return result\n",
    "    \n",
    "BASELINE_PATH = './baselines.json'\n",
    "jm = JM()\n",
    "parsed_jobs = jm._parse_tasks(BASELINE_PATH)\n",
    "pprint(parsed_jobs)\n",
    "print(len(parsed_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "d  = {\"1\":'asd', \"2\":\"sda\",\"3\":\"sad\"}\n",
    "for k in d:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
